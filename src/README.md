# ðŸ§  `src/` â€” Core Source Code

This directory contains all the main components of the **FineSign** system â€” from dataset loaders and video transforms to model architectures, trainers, and inference utilities.  
Itâ€™s designed for **clarity**, **reusability**, and **scalability**, following modular design principles inspired by large-scale ML systems.


## ðŸ“ Directory Overview
```php
src/
â”œâ”€ datasets/   # Handles data loading and preprocessing
â”œâ”€ transforms/ # Augmentation and temporal/spatial transforms
â”œâ”€ models/     # Backbone architectures, heads, and graph modules
â”œâ”€ trainers/   # Training logic and Hugging Face wrapper
â”œâ”€ utils/      # General-purpose helpers and logging tools
â”œâ”€ inference/  # Inference and evaluation pipeline
â””â”€ export/     # Model export to ONNX / TorchScript
```

## ðŸ§© Modules

### ðŸ—‚ï¸ `datasets/`
Handles dataset creation and frame/pose sampling for training and evaluation.

- **`base_dataset.py`** â€“ Abstract dataset class defining a unified API (`__getitem__`, `__len__`).
- **`video_dataset.py`** â€“ Loads RGB video frames, applies temporal sampling and augmentation.
- **`pose_dataset.py`** â€“ Loads pose/keypoint data (e.g., from MediaPipe or OpenPose) for multimodal fusion.

All datasets return a standardized dictionary:
```python
{
  "video": Tensor,       # [T, C, H, W]
  "pose": Optional[Tensor],
  "label": int,
}
```

### ðŸŽžï¸ `transforms/`
Implements preprocessing and data augmentation logic for both video and pose data.
- `video_transforms.py` â€“ Temporal sampling, frame normalization, resize, random crop, horizontal flip.
- `pose_transforms.py` â€“ Normalization and geometric jittering for keypoint coordinates.

Supports chaining via `torchvision.transforms.Compose`.

### ðŸ§  `models/`
Contains the main neural architectures.
- `backbones/`
    - `videomae_backbone.py` â€“ Transformer-based masked autoencoder for video representation.
    `timesformer_backbone.py` â€“ TimeSformer model (space-time attention).
- `heads/`
    - l`inear_head.py` â€“ Lightweight classification head.
    - `fusion_head.py` â€“ Fuses features from multiple modalities (video + pose).
- `graph/`
    - `stgcn.py` â€“ Spatial-Temporal Graph Convolutional Network for structured pose learning.
### âš™ï¸ `trainers/`
Training and fine-tuning logic.
- `trainer.py` â€“ Main training loop supporting checkpointing, resuming, mixed precision, and evaluation.
- `hf_trainer_wrapper.py` â€“ Optional integration with the Hugging Face Trainer API for faster experimentation.

Logging integrates with W&B or MLflow through `utils/logger.py`.

### ðŸ§° utils/
Utility scripts and helpers.
- `metrics.py` â€“ Accuracy, F1-score, confusion matrix computation.
- `logger.py` â€“ Abstract logger supporting W&B or MLflow.
- `seed.py` â€“ Ensures reproducibility by fixing seeds across NumPy, PyTorch, etc.
- `checkpoints.py` â€“ Save/load checkpoint management.

### ðŸ” `inference/`
Handles single-video prediction and batch evaluation.
- `infer.py`
    - Loads a trained model and runs forward inference.
    - Accepts video file or frame folder.
    - Outputs top-k predictions and confidence scores.

### ðŸ“¦ `export/`
Utilities for exporting trained models for deployment.
- `export_onnx.py` â€“ Converts PyTorch model â†’ ONNX or TorchScript for production environments.
    - Supports dynamic axes and half-precision export.
    - Example:
    ```
    python -m src.export.export_onnx \
        --checkpoint checkpoints/best_model.pt \
        --output onnx/finesign.onnx 
    ```


## APPENDIX - Design Principles
- Modular & Extensible: Add new backbones, datasets, or transforms with minimal coupling.
- Reproducible: Consistent training/evaluation through config-driven architecture.
- MLOps-Ready: Integrates smoothly with DVC, MLflow, and CI/CD pipelines.
- Research-Grade: Clear structure supports custom experimentation and ablations.


---
> *`src/` is the brain of project â€” where every frame, node, and signal becomes language*